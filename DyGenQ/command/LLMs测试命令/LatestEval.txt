------在LLMs上 LatestEval 上测试------------
python 05CQ_Test_Eval.py \
    --prompt ./promptEn/CQTest.txt \
    --dataset ./LatestEval/output/latesteval_mmlu_sample_300.json \
    --model gpt3 \
    --output ./output/CQLatestEval_gpt3_Answer.json

python 08countScoreCQ.py \
    --json ./output/CQLatestEval_gpt3_Answer.json
479/600=0.798
80.5
488/600=0.813
80.6±0.8

python 05CQ_Test_Eval.py \
    --prompt ./promptEn/CQTest.txt \
    --dataset ./LatestEval/output/latesteval_mmlu_sample_300.json \
    --model gpt4 \
    --output ./output/CQLatestEval_gpt4_Answer.json
    
python 08countScoreCQ.py \
    --json ./output/CQLatestEval_gpt4_Answer.json
524/600=0.873
526/600=0.877
521/600=0.868
87.3±0.5

python 05CQ_Test_Eval.py \
    --prompt ./promptEn/CQTest.txt \
    --dataset ./LatestEval/output/latesteval_mmlu_sample_300.json \
    --model doub \
    --output ./output/CQLatestEval_doub_Answer.json
    
python 08countScoreCQ.py \
    --json ./output/CQLatestEval_doub_Answer.json
530/596=0.889
530/596=0.889
529/596=0.888
88.9±0.0


python 05CQ_Test_Eval.py \
    --prompt ./promptEn/CQTest.txt \
    --dataset ./LatestEval/output/latesteval_mmlu_sample_300.json \
    --model qwen \
    --output ./output/CQLatestEval_qwen_Answer.json
    
python 08countScoreCQ.py \
    --json ./output/CQLatestEval_qwen_Answer.json
540/599=0.902
538/599=0.898
540/599=0.902
90.0±0.2

python 05CQ_Test_Eval.py \
    --prompt ./promptEn/CQTest.txt \
    --dataset ./LatestEval/output/latesteval_mmlu_sample_300.json \
    --model claude3 \
    --output ./output/CQLatestEval_claude3_Answer.json
    
python 08countScoreCQ.py \
    --json ./output/CQLatestEval_claude3_Answer.json
495/600=0.825
512/598=0.856
500/600=0.833
84.1±1.6

python 05CQ_Test_Eval.py \
    --prompt ./promptEn/CQTest.txt \
    --dataset ./LatestEval/output/latesteval_mmlu_sample_300.json \
    --model glm4 \
    --output ./output/CQLatestEval_glm4_Answer.json
    
python 08countScoreCQ.py \
    --json ./output/CQLatestEval_glm4_Answer.json
476/600=0.793
466/600=0.777
467/600=0.778
78.5±0.8

python 05CQ_Test_Eval.py \
    --prompt ./promptEn/CQTest.txt \
    --dataset ./LatestEval/output/latesteval_mmlu_sample_300.json \
    --model deepseek \
    --output ./output/CQLatestEval_deepseek_Answer.json
    
python 08countScoreCQ.py \
    --json ./output/CQLatestEval_deepseek_Answer.json
550/600=0.917✅
551/600=0.918
266/287=0.927

------在LLMs上 LatestEval 上测试 添加静态prompt数据------------
python 05CQ_Test_Eval_addStatic.py \
    --prompt ./promptEn/CQTestAddStatic.txt \
    --dataset ./LatestEval/output/latesteval_mmlu_sample_300.json \
    --datasetStatic ./dataset/formatDataset/mmlu/mmlu_sample_300.json \
    --model gpt3 \
    --output ./output/CQLatestEval_AddStatic_gpt3_Answer.json

python 08countScoreCQ.py \
    --json ./output/CQLatestEval_AddStatic_gpt3_Answer.json
508/600=0.847
84.7
509/600=0.848
84.7±0.0


python 05CQ_Test_Eval_addStatic.py \
    --prompt ./promptEn/CQTestAddStatic.txt \
    --dataset ./LatestEval/output/latesteval_mmlu_sample_300.json \
    --datasetStatic ./dataset/formatDataset/mmlu/mmlu_sample_300.json \
    --model gpt4 \
    --output ./output/CQLatestEval_AddStatic_gpt4_Answer.json

python 08countScoreCQ.py \
    --json ./output/CQLatestEval_AddStatic_gpt4_Answer.json
549/600=0.915
546/600=0.910
551/600=0.918
91.4±0.4

python 05CQ_Test_Eval_addStatic.py \
    --prompt ./promptEn/CQTestAddStatic.txt \
    --dataset ./LatestEval/output/latesteval_mmlu_sample_300.json \
    --datasetStatic ./dataset/formatDataset/mmlu/mmlu_sample_300.json \
    --model doub \
    --output ./output/CQLatestEval_AddStatic_doub_Answer.json

python 08countScoreCQ.py \
    --json ./output/CQLatestEval_AddStatic_doub_Answer.json
565/598=0.945
567/599=0.947
566/598=0.946
94.6±0.1


python 05CQ_Test_Eval_addStatic.py \
    --prompt ./promptEn/CQTestAddStatic.txt \
    --dataset ./LatestEval/output/latesteval_mmlu_sample_300.json \
    --datasetStatic ./dataset/formatDataset/mmlu/mmlu_sample_300.json \
    --model qwen \
    --output ./output/CQLatestEval_AddStatic_qwen_Answer.json

python 08countScoreCQ.py \
    --json ./output/CQLatestEval_AddStatic_qwen_Answer.json
574/598=0.960
576/598=0.963
573/598=0.958
96.1±0.3



python 05CQ_Test_Eval_addStatic.py \
    --prompt ./promptEn/CQTestAddStatic.txt \
    --dataset ./LatestEval/output/latesteval_mmlu_sample_300.json \
    --datasetStatic ./dataset/formatDataset/mmlu/mmlu_sample_300.json \
    --model claude3 \
    --output ./output/CQLatestEval_AddStatic_claude3_Answer.json

python 08countScoreCQ.py \
    --json ./output/CQLatestEval_AddStatic_claude3_Answer.json
525/600=0.875
531/600=0.885
525/600=0.875
88.0±0.5


python 05CQ_Test_Eval_addStatic.py \
    --prompt ./promptEn/CQTestAddStatic.txt \
    --dataset ./LatestEval/output/latesteval_mmlu_sample_300.json \
    --datasetStatic ./dataset/formatDataset/mmlu/mmlu_sample_300.json \
    --model glm4 \
    --output ./output/CQLatestEval_AddStatic_glm4_Answer.json

python 08countScoreCQ.py \
    --json ./output/CQLatestEval_AddStatic_glm4_Answer.json
496/572=0.867
494/572=0.864
494/572=0.864
86.5±0.1


python 05CQ_Test_Eval_addStatic.py \
    --prompt ./promptEn/CQTestAddStatic.txt \
    --dataset ./LatestEval/output/latesteval_mmlu_sample_300.json \
    --datasetStatic ./dataset/formatDataset/mmlu/mmlu_sample_300.json \
    --model deepseek \
    --output ./output/CQLatestEval_AddStatic_deepseek_Answer.json

python 08countScoreCQ.py \
    --json ./output/CQLatestEval_AddStatic_deepseek_Answer.json
569/600=0.948✅
567/600=0.945
493/524=0.941